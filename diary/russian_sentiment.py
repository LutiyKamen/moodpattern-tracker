import re
import os


def _load_stopwords():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"""
    return {
        '—ç—Ç–æ', '–≤–æ—Ç', '–∫–∞–∫–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '—Å–µ–≥–æ–¥–Ω—è', '–∑–∞–≤—Ç—Ä–∞', '–≤—á–µ—Ä–∞',
        '–ø—Ä–æ—Å—Ç–æ', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–±—É–¥–µ—Ç', '–µ—Å—Ç—å', '–±—ã–ª', '–±—ã–ª–∞',
        '–±—ã–ª–æ', '–±—ã–ª–∏', '–≤–µ—Å—å', '–≤—Å–µ', '–≤—Å—ë', '–≤—Å–µ–≥–æ', '–≤—Å–µ–º', '—Å–∞–º', '—Å–∞–º–∞',
        '—Å–∞–º–æ', '—Å–∞–º–∏', '—Ä–∞–∑', '–¥–≤–∞', '—Ç—Ä–∏', '–≥–æ–¥', '–≥–æ–¥–∞', '–ª–µ—Ç',
        '–∫–∞–∫', '—Ç–∞–∫', '—Ç–∞–º', '–∑–¥–µ—Å—å', '—Ç—É—Ç', '–≥–¥–µ', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞',
        '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '—Å–∫–æ–ª—å–∫–æ', '–∫–æ–≥–¥–∞', '—á—Ç–æ', '—á—Ç–æ–±—ã', '–µ—Å–ª–∏'
    }


def _load_categories():
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–ª–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    return {
        # –≠–º–æ—Ü–∏–∏
        'emotion_positive': {'—Å—á–∞—Å—Ç–ª–∏–≤', '—Ä–∞–¥–æ—Å—Ç', '–≤–µ—Å–µ–ª', '–¥–æ–≤–æ–ª–µ–Ω'},
        'emotion_negative': {'–≥—Ä—É—Å—Ç–Ω', '–ø–µ—á–∞–ª—å–Ω', '—Ç–æ—Å–∫–ª–∏–≤', '–∑–ª', '—Å–µ—Ä–¥–∏—Ç'},

        # –°–æ—Å—Ç–æ—è–Ω–∏—è
        'state_positive': {'–∑–¥–æ—Ä–æ–≤', '—Å–∏–ª—å–Ω', '—ç–Ω–µ—Ä–≥–∏—á–Ω', '–±–æ–¥—Ä'},
        'state_negative': {'–±–æ–ª—å–Ω', '—É—Å—Ç–∞–ª', '—Å–ª–∞–±', '—É—Ç–æ–º–ª–µ–Ω'},

        # –°–æ–±—ã—Ç–∏—è
        'event_positive': {'–ø—Ä–∞–∑–¥–Ω–∏–∫', '–ø–æ–¥–∞—Ä–æ–∫', '–Ω–∞–≥—Ä–∞–¥–∞', '—É—Å–ø–µ—Ö'},
        'event_negative': {'–ø—Ä–æ–±–ª–µ–º', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '—Å—Å–æ—Ä', '–Ω–µ—É–¥–∞—á'},

        # –°–æ—Ü–∏–∞–ª—å–Ω–æ–µ
        'social_positive': {'–¥—Ä—É–≥', '—Å–µ–º—å—è', '–ª—é–±–æ–≤—å', '–ø–æ–¥–¥–µ—Ä–∂–∫'},
        'social_negative': {'–æ–¥–∏–Ω–æ–∫', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '—Å—Å–æ—Ä', '–∏–∑–º–µ–Ω'},
    }


def _get_default_words(filename):
    """–ë–∞–∑–æ–≤—ã–µ –Ω–∞–±–æ—Ä—ã —Å–ª–æ–≤ –µ—Å–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"""
    defaults = {
        'positive_ru.txt': {
            '—Ö–æ—Ä–æ—à', '–æ—Ç–ª–∏—á–Ω', '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω', '–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω',
            '—Å—á–∞—Å—Ç–ª–∏–≤', '—Ä–∞–¥–æ—Å—Ç', '–≤–µ—Å–µ–ª', '–¥–æ–≤–æ–ª–µ–Ω', '—É—Å–ø–µ—à–Ω'
        },
        'negative_ru.txt': {
            '–ø–ª–æ—Ö', '—É–∂–∞—Å–Ω', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω', '–≥—Ä—É—Å—Ç–Ω', '–ø–µ—á–∞–ª—å–Ω',
            '–∑–ª', '—Å–µ—Ä–¥–∏—Ç', '–±–æ–ª—å–Ω', '—É—Å—Ç–∞–ª', '–ø—Ä–æ–±–ª–µ–º'
        },
        'intensifiers_ru.txt': {'–æ—á–µ–Ω—å', '—Å–∏–ª—å–Ω–æ', '–∫—Ä–∞–π–Ω–µ', '—á—Ä–µ–∑–≤—ã—á–∞–π–Ω–æ'},
        'negations_ru.txt': {'–Ω–µ', '–Ω–∏', '–Ω–µ—Ç', '–±–µ–∑'}
    }
    return defaults.get(filename, set())


def preprocess_text(text):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞: –æ—á–∏—Å—Ç–∫–∞, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"""
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = text.lower()

    # –ó–∞–º–µ–Ω—è–µ–º —ë –Ω–∞ –µ
    text = text.replace('—ë', '–µ')

    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)

    return text


def stem_word(word):
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å—Ç–µ–º–º–∏–Ω–≥ —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤"""
    if len(word) < 4:
        return word

    # –û–±—â–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
    endings = ['—ã–π', '–∏–π', '–æ–π', '–∞—è', '—è—è', '–æ–µ', '–µ–µ', '—ã–µ', '–∏–µ',
               '–æ—Å—Ç—å', '–∞—Ü–∏—è', '–µ–Ω–∏–µ', '–∞–Ω—å–µ', '—Å—Ç–≤–æ', '–∏–∑–º',
               '–Ω–Ω–æ', '–µ–Ω–Ω–æ', '–∞–ª—å–Ω–æ']

    for ending in endings:
        if word.endswith(ending):
            return word[:-len(ending)]

    return word


def _score_to_sentiment(score):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —á–∏—Å–ª–æ–≤—É—é –æ—Ü–µ–Ω–∫—É –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ"""
    if score > 7:
        return "–æ—á–µ–Ω—å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π"
    elif score > 3:
        return "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π"
    elif score > 1:
        return "—Å–ª–µ–≥–∫–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π"
    elif score > -1:
        return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π"
    elif score > -3:
        return "—Å–ª–µ–≥–∫–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π"
    elif score > -7:
        return "–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π"
    else:
        return "–æ—á–µ–Ω—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π"


class AdvancedRussianSentimentAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""

    def __init__(self, data_dir='diary/sentiment_data'):
        self.data_dir = data_dir

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏
        self.positive_words = self._load_wordlist('positive_ru.txt')
        self.negative_words = self._load_wordlist('negative_ru.txt')
        self.intensifiers = self._load_wordlist('intensifiers_ru.txt')
        self.negations = self._load_wordlist('negations_ru.txt')

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        self.stopwords = _load_stopwords()

        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–ª–æ–≤
        self.weights = {
            'positive': 1.0,
            'negative': -1.0,
            'intensifier': 1.5,  # —É—Å–∏–ª–∏–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ
            'negation': -1.0,  # –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ
        }

        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        self.emotional_patterns = [
            (r'!{2,}', 1.3),  # –í–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è !!
            (r'\?{2,}', -0.5),  # –ú–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤ ??
            (r'\.{3,}', -0.7),  # –ú–Ω–æ–≥–æ—Ç–æ—á–∏–µ ...
            (r'[A-Z–ê-–Ø]{4,}', 0.8),  # –ö–ê–ü–°–õ–û–ö
            (r'[‚ô•‚ô°‚ù§Ô∏èüíïüíñ]', 1.2),  # –°–µ—Ä–¥–µ—á–∫–∏
            (r'[üòäüòÇü§£üòçü•∞]', 1.5),  # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —ç–º–æ–¥–∑–∏
            (r'[üò¢üò≠üòîüòûüò†]', -1.5),  # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —ç–º–æ–¥–∑–∏
        ]

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–ª–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        self.word_categories = _load_categories()

    def _load_wordlist(self, filename):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
        filepath = os.path.join(self.data_dir, filename)
        words = set()

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip().lower()
                    if word and not word.startswith('#'):
                        words.add(word)
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(words)} —Å–ª–æ–≤ –∏–∑ {filename}")
        except FileNotFoundError:
            print(f"‚ö† –§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä")
            words = _get_default_words(filename)

        return words

    def tokenize(self, text):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ä—É—Å—Å–∫–æ–≥–æ"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ
        tokens = re.findall(r'\b[–∞-—è—ë]+\b|[!?.,;:]+', text)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        tokens = [t for t in tokens if t not in self.stopwords and len(t) > 2]

        return tokens

    def analyze_sentiment(self, text):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        if not text or len(text.strip()) < 3:
            return 0.0, []

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text = preprocess_text(text)

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = self.tokenize(text)

        if not tokens:
            return 0.0, []

        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        score = 0.0
        sentiment_words = []
        i = 0

        while i < len(tokens):
            token = tokens[i]
            token_score = 0.0
            multiplier = 1.0

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–∏–ª–∏—Ç–µ–ª–∏
            if token in self.intensifiers:
                if i + 1 < len(tokens):
                    next_token = tokens[i + 1]
                    # –£—Å–∏–ª–∏—Ç–µ–ª—å –≤–ª–∏—è–µ—Ç –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ
                    multiplier = self.weights['intensifier']
                    i += 1
                    token = next_token

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Ä–∏—Ü–∞–Ω–∏—è
            if token in self.negations:
                if i + 1 < len(tokens):
                    next_token = tokens[i + 1]
                    # –û—Ç—Ä–∏—Ü–∞–Ω–∏–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ
                    multiplier *= self.weights['negation']
                    i += 1
                    token = next_token

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞
            stemmed = stem_word(token)

            if stemmed in self.positive_words or any(pos in stemmed for pos in self.positive_words):
                token_score = self.weights['positive']
                sentiment_words.append((token, token_score * multiplier))
            elif stemmed in self.negative_words or any(neg in stemmed for neg in self.negative_words):
                token_score = self.weights['negative']
                sentiment_words.append((token, token_score * multiplier))

            # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É —Å–ª–æ–≤–∞ (–¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –æ–±—ã—á–Ω–æ –∑–Ω–∞—á–∏–º–µ–µ)
            if len(token) > 6:
                token_score *= 1.1

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å
            token_score *= multiplier

            score += token_score
            i += 1

        # –£—á–∏—Ç—ã–≤–∞–µ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        pattern_score = self._analyze_patterns(text)
        score += pattern_score

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if sentiment_words:
            avg_score = score / len(sentiment_words)
            normalized_score = avg_score * 10  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–æ -10..10
        else:
            normalized_score = 0.0

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        normalized_score = max(-10.0, min(10.0, normalized_score))

        return round(normalized_score, 2), sentiment_words

    def _analyze_patterns(self, text):
        """–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        pattern_score = 0.0

        for pattern, weight in self.emotional_patterns:
            matches = re.findall(pattern, text)
            if matches:
                pattern_score += len(matches) * weight

        # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
        words_count = len(text.split())
        if words_count > 0:
            # –ë–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–º–µ—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å
            length_factor = min(2.0, words_count / 50)
            pattern_score *= length_factor

        return pattern_score

    def get_detailed_analysis(self, text):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        score, sentiment_words = self.analyze_sentiment(text)

        analysis = {
            'overall_score': score,
            'sentiment': _score_to_sentiment(score),
            'word_count': len(text.split()),
            'sentiment_words_count': len(sentiment_words),
            'sentiment_words': sentiment_words,
            'positive_words': [w for w, s in sentiment_words if s > 0],
            'negative_words': [w for w, s in sentiment_words if s < 0],
            'intensity': abs(score),
        }

        return analysis


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
analyzer = AdvancedRussianSentimentAnalyzer()


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
def analyze_russian_sentiment(text):
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    score, _ = analyzer.analyze_sentiment(text)
    return score